{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16520d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from trainer.models.common.transformer import FeedForward, MultiHeadSelfAttentionLayer\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"SASRec embedding is composed of a positional embedding layer and a normal embedding layer\n",
    "\n",
    "    Input shape\n",
    "      - token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_embedding, seq_length=50, dim=50, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        assert seq_length % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "        self.length = seq_length\n",
    "        self.dim = dim\n",
    "        self.token_emb = token_embedding\n",
    "        self.position_emb = tf.keras.layers.Embedding(\n",
    "            input_dim=seq_length, output_dim=dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        length = tf.shape(inputs)[1]\n",
    "        embedded_tokens = self.token_emb(inputs)\n",
    "        embedded_positions = self.position_emb(tf.range(length))\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        embedded_tokens *= tf.math.sqrt(tf.cast(self.dim, tf.float32))\n",
    "        return embedded_tokens + embedded_positions[tf.newaxis, :, :]\n",
    "\n",
    "    # Pass mask from token_emb, https://www.tensorflow.org/guide/keras/understanding_masking_and_padding#supporting_masking_in_your_custom_layers\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_emb.compute_mask(inputs, mask=mask)\n",
    "\n",
    "\n",
    "class SASRecBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"SASRec block is a stack of self attention layer + MLP + layer norm + residual layers\n",
    "\n",
    "    Input shape\n",
    "      - token embedding 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_num=1, dim=50, dropout=0.1, **kwargs):\n",
    "        super(SASRecBlock, self).__init__(**kwargs)\n",
    "        self.head_num = head_num\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = MultiHeadSelfAttentionLayer(\n",
    "            head_num=head_num, key_dim=dim, dropout=dropout\n",
    "        )\n",
    "        self.ff = FeedForward(ff_dim=dim, dropout=dropout, model_dim=dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inputs = self.norm(inputs, training=training)\n",
    "        inputs = self.add(\n",
    "            [\n",
    "                inputs,\n",
    "                self.dropout1(\n",
    "                    self.attention(\n",
    "                        inputs, inputs, inputs, training=training, use_causal_mask=True\n",
    "                    ),\n",
    "                    training=training\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # must enable causal mask\n",
    "        return self.ff(\n",
    "            inputs,\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"head_num\": self.head_num,\n",
    "                \"dim\": self.dim,\n",
    "                \"dropout\": self.dropout,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class SASRec(tf.keras.layers.Layer):\n",
    "    \"\"\"SASRec model is a stack of self attention layers\n",
    "\n",
    "    Input shape\n",
    "      - sequential token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "      - positive token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "      - negative token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, 2)``.\n",
    "\n",
    "    References\n",
    "        - [Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        head_num=1,\n",
    "        block_num=2,\n",
    "        seq_length=50,\n",
    "        dim=50,\n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SASRec, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.head_num = head_num\n",
    "        self.block_num = block_num\n",
    "        self.seq_length = seq_length\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        # will be reused to general pos and neg embeddings\n",
    "        self.token_emb = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=dim, mask_zero=True\n",
    "        )\n",
    "        self.positional_emb = PositionalEmbedding(\n",
    "            self.token_emb, seq_length=seq_length, dim=dim\n",
    "        )\n",
    "        self.sas_blocks = [\n",
    "            SASRecBlock(head_num=head_num, dim=dim, dropout=dropout)\n",
    "            for _ in range(block_num)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_token, pos, neg = inputs\n",
    "        # shape [batch_size, token_length, dim]\n",
    "        input_emb = self.positional_emb(input_token)\n",
    "        pos_emb = self.token_emb(pos)\n",
    "        neg_emb = self.token_emb(neg)\n",
    "        for sas_block in self.sas_blocks:\n",
    "            output_emb = sas_block(input_emb, training=training)\n",
    "        # shape [batch_size, token_length, 1]\n",
    "        pos_logits = tf.reduce_sum(output_emb * pos_emb, axis=-1, keepdims=True)\n",
    "        neg_logits = tf.reduce_sum(output_emb * neg_emb, axis=-1, keepdims=True)\n",
    "        # shape [batch_size, token_length, 2]\n",
    "        return tf.concat([pos_logits, neg_logits], axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"block_num\": self.block_num,\n",
    "                \"seq_length\": self.seq_length,\n",
    "                \"head_num\": self.head_num,\n",
    "                \"dim\": self.dim,\n",
    "                \"dropout\": self.dropout,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ef1d6851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "tokens = tf.reshape(tf.range(9), [1, 9])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7a7f944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32)>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = tf.reshape(tf.range(1, 10), [1, 9])\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d8a39b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[18, 83, 81, 19, 85, 81, 76, 31, 48]], dtype=int32)>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = tf.random.uniform([1, 9], minval=10, maxval=100, dtype=tf.dtypes.int32)\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d4942b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[18, 83, 81, 19, 85, 81, 76, 31, 48]], dtype=int32)>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_rec = SASRec(vocab_size)\n",
    "(tokens, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "abc31406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9, 2), dtype=float32, numpy=\n",
       "array([[[-0.23222207, -0.02898993],\n",
       "        [-0.19377749, -0.22032568],\n",
       "        [-0.08979567,  0.26086435],\n",
       "        [ 0.39162612,  0.02455861],\n",
       "        [-0.10788958, -0.00203439],\n",
       "        [ 0.12806168,  0.2689228 ],\n",
       "        [ 0.15980572, -0.06050337],\n",
       "        [-0.15532795,  0.00756386],\n",
       "        [-0.27716565, -0.01334527]]], dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_rec((tokens, pos, neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "56b0881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(label, pred):\n",
    "    return tf.reduce_sum(\n",
    "        - tf.math.log(tf.math.sigmoid(pred[:,:,0]) + 1e-24) -\n",
    "        tf.math.log(1 - tf.math.sigmoid(pred[:,:,1]) + 1e-24)\n",
    "    ) / tf.cast(tf.reduce_sum(label), tf.float32)\n",
    "\n",
    "\n",
    "def auc(label, pred):\n",
    "    return tf.reduce_sum(\n",
    "            ((tf.math.sign(pred[:,:,0] - pred[:,:,1]) + 1) / 2)\n",
    "        ) / tf.cast(tf.reduce_sum(label), tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "813af08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRecModel(tf.keras.Model):\n",
    "\n",
    "    def call(self, inputs, training=False) -> tf.Tensor:\n",
    "        logits = sas_rec(tf.split(inputs, 3, axis=1), training=training)\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b0f33840",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_rec_model = SASRecModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "13e89c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_rec_model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.001,\n",
    "            ),\n",
    "            metrics=[auc],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e645e966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9, 2), dtype=int32, numpy=\n",
       "array([[[1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]]], dtype=int32)>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = tf.repeat([[[1, 0]]], repeats=[9], axis=1)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "03b0c60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.reduce_sum(label), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1c800cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 9), dtype=int32, numpy=\n",
       "array([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [18, 83, 81, 19, 85, 81, 76, 31, 48]]], dtype=int32)>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([tokens, pos, neg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9e28761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((tf.stack([tokens, pos, neg], axis=1), label)).repeat(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e48f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3118/10000 [========>.....................] - ETA: 24s - loss: 1.3575 - auc: 0.5183"
     ]
    }
   ],
   "source": [
    "sas_rec_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe891e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

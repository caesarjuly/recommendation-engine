{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "abd7eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from trainer.models.common.transformer import FeedForward, MultiHeadSelfAttentionLayer\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"SASRec embedding is composed of a positional embedding layer and a normal embedding layer\n",
    "\n",
    "    Input shape\n",
    "      - token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_embedding, seq_length=50, dim=50, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        assert seq_length % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "        self.length = seq_length\n",
    "        self.dim = dim\n",
    "        self.token_emb = token_embedding\n",
    "        self.position_emb = tf.keras.layers.Embedding(\n",
    "            input_dim=seq_length, output_dim=dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        length = tf.shape(inputs)[1]\n",
    "        embedded_tokens = self.token_emb(inputs)\n",
    "        embedded_positions = self.position_emb(tf.range(length))\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        embedded_tokens *= tf.math.sqrt(tf.cast(self.dim, tf.float32))\n",
    "        return embedded_tokens + embedded_positions[tf.newaxis, :, :]\n",
    "\n",
    "    # Pass mask from token_emb, https://www.tensorflow.org/guide/keras/understanding_masking_and_padding#supporting_masking_in_your_custom_layers\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_emb.compute_mask(inputs, mask=mask)\n",
    "\n",
    "\n",
    "class SASRecBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"SASRec block is a stack of self attention layer + MLP + layer norm + residual layers\n",
    "\n",
    "    Input shape\n",
    "      - token embedding 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_num=1, dim=50, dropout=0.1, **kwargs):\n",
    "        super(SASRecBlock, self).__init__(**kwargs)\n",
    "        self.head_num = head_num\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = MultiHeadSelfAttentionLayer(\n",
    "            head_num=head_num, key_dim=dim, dropout=dropout\n",
    "        )\n",
    "        self.ff = FeedForward(ff_dim=dim, dropout=dropout, model_dim=dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # must enable causal mask\n",
    "        return self.ff(\n",
    "            self.attention(inputs, inputs, inputs, training=training, use_causal_mask=True),\n",
    "            training=training,\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"head_num\": self.head_num,\n",
    "                \"dim\": self.dim,\n",
    "                \"dropout\": self.dropout,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class SASRec(tf.keras.layers.Layer):\n",
    "    \"\"\"SASRec model is a stack of self attention layers\n",
    "\n",
    "    Input shape\n",
    "      - sequential token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "      - positive token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "      - negative token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, 2)``.\n",
    "\n",
    "    References\n",
    "        - [Self-Attentive Sequential Recommendation](https://arxiv.org/pdf/1808.09781.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        head_num=1,\n",
    "        block_num=2,\n",
    "        seq_length=50,\n",
    "        dim=50,\n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(SASRec, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.head_num = head_num\n",
    "        self.block_num = block_num\n",
    "        self.seq_length = seq_length\n",
    "        self.dim = dim\n",
    "        self.dropout = dropout\n",
    "        # will be reused to general pos and neg embeddings\n",
    "        self.token_emb = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=dim, mask_zero=True\n",
    "        )\n",
    "        self.positional_emb = PositionalEmbedding(\n",
    "            self.token_emb, seq_length=seq_length, dim=dim\n",
    "        )\n",
    "        self.sas_blocks = [\n",
    "            SASRecBlock(head_num=head_num, dim=dim, dropout=dropout)\n",
    "            for _ in range(block_num)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_token, pos, neg = inputs\n",
    "        # shape [batch_size, token_length, dim]\n",
    "        input_emb = self.positional_emb(input_token)\n",
    "        pos_emb = self.token_emb(pos)\n",
    "        neg_emb = self.token_emb(neg)\n",
    "        for sas_block in self.sas_blocks:\n",
    "            output_emb = sas_block(input_emb, training=training)\n",
    "        # shape [batch_size, token_length, 1]\n",
    "        pos_logits = tf.reduce_sum(output_emb * pos_emb, axis=-1, keepdims=True)\n",
    "        neg_logits = tf.reduce_sum(output_emb * neg_emb, axis=-1, keepdims=True)\n",
    "        # shape [batch_size, token_length, 2]\n",
    "        return tf.concat([pos_logits, neg_logits], axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"block_num\": self.block_num,\n",
    "                \"seq_length\": self.seq_length,\n",
    "                \"head_num\": self.head_num,\n",
    "                \"dim\": self.dim,\n",
    "                \"dropout\": self.dropout,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e344f3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10\n",
    "tokens = tf.reshape(tf.range(9), [1, 9])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c50f859e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = tf.reshape(tf.range(1, 10), [1, 9])\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "70033ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[3, 7, 4, 1, 3, 7, 4, 1, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = tf.random.uniform([1, 9], minval=0, maxval=10, dtype=tf.dtypes.int32)\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5def0e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[3, 7, 4, 1, 3, 7, 4, 1, 0]], dtype=int32)>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_rec = SASRec(vocab_size)\n",
    "(tokens, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b811878d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9, 2), dtype=float32, numpy=\n",
       "array([[[-0.30672818, -0.1411577 ],\n",
       "        [ 0.00989199,  0.00433109],\n",
       "        [-0.19873658, -0.23819302],\n",
       "        [-0.27444744, -0.39519155],\n",
       "        [-0.09321697, -0.3705861 ],\n",
       "        [ 0.14692208,  0.01091674],\n",
       "        [ 0.11229304, -0.5366032 ],\n",
       "        [ 0.18126515, -0.38814524],\n",
       "        [-0.01035482, -0.18661167]]], dtype=float32)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_rec((tokens, pos, neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "66913ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(label, pred):\n",
    "    return tf.reduce_sum(\n",
    "        - tf.math.log(tf.math.sigmoid(pred[:,:,0]) + 1e-24) -\n",
    "        tf.math.log(1 - tf.math.sigmoid(pred[:,:,1]) + 1e-24)\n",
    "    ) / tf.cast(tf.reduce_sum(label), tf.float32)\n",
    "\n",
    "\n",
    "def auc(label, pred):\n",
    "    return tf.reduce_sum(\n",
    "            ((tf.math.sign(pred[:,:,0] - pred[:,:,1]) + 1) / 2)\n",
    "        ) / tf.cast(tf.reduce_sum(label), tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4436cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRecModel(tf.keras.Model):\n",
    "\n",
    "    def call(self, inputs, training=False) -> tf.Tensor:\n",
    "        logits = sas_rec(tf.split(inputs, 3, axis=1), training=training)\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4553ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_rec_model = SASRecModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2e1afb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_rec_model.compile(\n",
    "            loss=loss,\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.001,\n",
    "            ),\n",
    "            metrics=[auc],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "96e90c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 9, 2), dtype=int32, numpy=\n",
       "array([[[1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]]], dtype=int32)>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = tf.repeat([[[1, 0]]], repeats=[9], axis=1)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "09d56373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.0>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.reduce_sum(label), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c434d0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 9), dtype=int32, numpy=\n",
       "array([[[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       "        [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [3, 7, 4, 1, 3, 7, 4, 1, 0]]], dtype=int32)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([tokens, pos, neg], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "844adc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((tf.stack([tokens, pos, neg], axis=1), label)).repeat(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "22e744f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 4s 3ms/step - loss: 1.4109 - auc: 3.4595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e646551f0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sas_rec_model.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c57f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

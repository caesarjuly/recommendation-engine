{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42914d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 14:10:47.417126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 14:10:47.494697: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools\n",
    "print(\"TensorFlow version: \", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd7115cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 14:11:09.215404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.227401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.228334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.230506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 14:11:09.232237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.233025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.233599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.566322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.566745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.567091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-31 14:11:09.567443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8629 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e7d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "\n",
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    "\n",
    "_VOCAB_SIZE = len(_VOCAB)\n",
    "\n",
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=_VOCAB,\n",
    "      key_dtype=tf.string,\n",
    "      values=tf.range(\n",
    "          tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
    "          value_dtype=tf.int64\n",
    "        ),\n",
    "      num_oov_buckets=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e91bebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[3, 22, 9, 24, 23, 4, 16, 5, 19, 6, 4],\n",
       "  [3, 18, 12, 15, 13, 4, 21, 28, 30, 27, 4]]>,\n",
       " <tf.RaggedTensor [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_combined, segments_ids = text.combine_segments(\n",
    "  trimmed,\n",
    "  start_of_sequence_id=_START_TOKEN, end_of_segment_id=_END_TOKEN)\n",
    "segments_combined, segments_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ccc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selector = text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42533149",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee8b0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "    # Input is a string Tensor of documents, shape [batch, 1].\n",
    "    text_a = features[\"text_a\"]\n",
    "    text_b = features[\"text_b\"]\n",
    "\n",
    "    # Tokenize segments to shape [num_sentences, (num_words)] each.\n",
    "    tokenizer = text.BertTokenizer(\n",
    "      vocab_table,\n",
    "      token_out_type=tf.int64)\n",
    "    segments = [tokenizer.tokenize(text).merge_dims(\n",
    "      1, -1) for text in (text_a, text_b)]\n",
    "\n",
    "    # Truncate inputs to a maximum length.\n",
    "    trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
    "    trimmed_segments = trimmer.trim(segments)\n",
    "\n",
    "    # Combine segments, get segment ids and add special tokens.\n",
    "    segments_combined, segment_ids = text.combine_segments(\n",
    "      trimmed_segments,\n",
    "      start_of_sequence_id=_START_TOKEN,\n",
    "      end_of_segment_id=_END_TOKEN)\n",
    "\n",
    "    # Apply dynamic masking task.\n",
    "    masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
    "      text.mask_language_model(\n",
    "        segments_combined,\n",
    "        random_selector,\n",
    "        mask_values_chooser,\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # Prepare and pad combined segment inputs\n",
    "    input_word_ids, input_mask = text.pad_model_inputs(\n",
    "    masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = text.pad_model_inputs(\n",
    "    segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "    # Prepare and pad masking task inputs\n",
    "    masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "    masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids, _ = text.pad_model_inputs(\n",
    "    masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "    model_inputs = {\n",
    "      \"input_word_ids\": input_word_ids,\n",
    "      \"input_mask\": input_mask,\n",
    "      \"input_type_ids\": input_type_ids,\n",
    "      \"masked_lm_ids\": masked_lm_ids,\n",
    "      \"masked_lm_positions\": masked_lm_positions,\n",
    "      \"masked_lm_weights\": masked_lm_weights,\n",
    "    }\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dd4d4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[ 3,  1,  9, 24,  4, 16,  5,  1],\n",
       "        [ 3, 18, 12, 15,  4, 21, 28,  1]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[22, 19,  0,  0,  0],\n",
       "        [28, 30,  0,  0,  0]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 7, 0, 0, 0],\n",
       "        [6, 7, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")\n",
    "\n",
    "next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4002ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Bert embedding is composed of a positional embedding layer, a segment embedding layer and a normal embedding layer\n",
    "\n",
    "    Input shape\n",
    "      - token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "      - segment index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, seq_length=128, dim=512, **kwargs):\n",
    "        super(BertEmbedding, self).__init__(**kwargs)\n",
    "        assert seq_length % 2 == 0, \"Output dimension needs to be an even integer\"\n",
    "        self.length = seq_length\n",
    "        self.dim = dim\n",
    "        self.token_emb = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=dim, mask_zero=True\n",
    "        )\n",
    "        self.position_emb = tf.keras.layers.Embedding(input_dim=seq_length, output_dim=dim)\n",
    "        self.segment_emb = tf.keras.layers.Embedding(input_dim=2, output_dim=dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        tokens = inputs[\"input_word_ids\"]\n",
    "        segments = inputs[\"input_type_ids\"]\n",
    "        length = tf.shape(tokens)[1]\n",
    "        embedded_tokens = self.token_emb(tokens)\n",
    "        embedded_segments = self.segment_emb(segments)\n",
    "        embedded_positions = self.position_emb(tf.range(length))\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        embedded_tokens *= tf.math.sqrt(tf.cast(self.dim, tf.float32))\n",
    "        return (\n",
    "            embedded_tokens + embedded_segments + embedded_positions[tf.newaxis, :, :]\n",
    "        )\n",
    "\n",
    "    # Pass mask from token_emb, https://www.tensorflow.org/guide/keras/understanding_masking_and_padding#supporting_masking_in_your_custom_layers\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs[\"input_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3e605d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertEmbedding(len(_VOCAB), _MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9bbc852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 512), dtype=float32, numpy=\n",
       "array([[[ 0.05752446,  0.75627196,  0.27433577, ...,  1.0190184 ,\n",
       "         -0.83793116, -0.3768208 ],\n",
       "        [ 0.04593604, -0.8231284 ,  1.1117262 , ...,  0.26253074,\n",
       "         -0.8496783 , -0.8961436 ],\n",
       "        [-0.11777939,  0.39778906,  0.95789903, ..., -0.38639542,\n",
       "          0.64713544, -0.4776726 ],\n",
       "        ...,\n",
       "        [-0.61908835, -0.43666652,  0.6317425 , ...,  0.34010994,\n",
       "         -0.7603367 ,  1.1500205 ],\n",
       "        [ 0.9602606 ,  0.16026199, -0.7066235 , ...,  0.38239995,\n",
       "         -0.60343444,  1.05717   ],\n",
       "        [-0.75959325, -0.202389  ,  1.0904251 , ...,  0.01723169,\n",
       "         -0.53571177, -0.68253744]],\n",
       "\n",
       "       [[ 0.05752446,  0.75627196,  0.27433577, ...,  1.0190184 ,\n",
       "         -0.83793116, -0.3768208 ],\n",
       "        [-0.59359604, -0.35077828,  0.62417734, ...,  0.4131376 ,\n",
       "         -0.7492717 ,  1.0248871 ],\n",
       "        [-0.61279154,  0.91318214,  1.0679604 , ...,  1.039956  ,\n",
       "         -0.2652515 ,  0.6798283 ],\n",
       "        ...,\n",
       "        [-1.045168  ,  0.13650282,  0.04114823, ...,  0.7895857 ,\n",
       "          0.21280205,  0.16367394],\n",
       "        [-0.42934346,  0.2949735 , -0.9319048 , ...,  0.33906403,\n",
       "          0.08882743,  0.73583996],\n",
       "        [-0.5731357 , -0.39162955,  0.62727255, ...,  0.3941168 ,\n",
       "         -0.77726644,  1.1148547 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "857ee951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trainer.models.common.transformer import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1d92073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(tf.keras.layers.Layer):\n",
    "    \"\"\"Bert model is a stack of multiple layers of encoders\n",
    "\n",
    "    Input shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        layer_num=6,\n",
    "        model_dim=512,\n",
    "        ff_dim=2048,\n",
    "        dropout=0.1,\n",
    "        head_num=8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Bert, self).__init__(**kwargs)\n",
    "        self.emb = BertEmbedding(\n",
    "            vocab_size=vocab_size, seq_length=seq_length, dim=model_dim\n",
    "        )\n",
    "        self.encoders = [\n",
    "            Encoder(\n",
    "                model_dim=model_dim, ff_dim=ff_dim, dropout=dropout, head_num=head_num\n",
    "            )\n",
    "            for _ in range(layer_num)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # shape [batch_size, token_length]\n",
    "        emb = self.emb(inputs, training=training)\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        for encoder in self.encoders:\n",
    "            emb = encoder(emb, training=training)\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        return emb\n",
    "\n",
    "\n",
    "class BertMLM(tf.keras.layers.Layer):\n",
    "    \"\"\"Masked language model simply mask some percentage of the input tokens at random, and then predict those masked tokens\n",
    "\n",
    "    Input shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        layer_num=6,\n",
    "        model_dim=512,\n",
    "        ff_dim=2048,\n",
    "        dropout=0.1,\n",
    "        head_num=8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(BertMLM, self).__init__(**kwargs)\n",
    "\n",
    "        self.emb = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            seq_length=seq_length,\n",
    "            layer_num=layer_num,\n",
    "            model_dim=model_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            dropout=dropout,\n",
    "            head_num=head_num,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            vocab_size, activation=tf.keras.activations.softmax\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        emb = self.dropout(self.emb(inputs, training=training))\n",
    "        # shape [batch_size, token_length, vocab_size_logits]\n",
    "        emb = self.dense(emb)\n",
    "        # gather the corresponding logits per the masked_lm_positions\n",
    "        return tf.gather(emb, inputs[\"masked_lm_positions\"], axis=1, batch_dims=1)\n",
    "\n",
    "\n",
    "class BertNSP(tf.keras.layers.Layer):\n",
    "    \"\"\"Next sentence predition use the CLS token embedding for prediction\n",
    "\n",
    "    Input shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, 1)``.\n",
    "\n",
    "    References\n",
    "        - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        layer_num=6,\n",
    "        model_dim=512,\n",
    "        ff_dim=2048,\n",
    "        dropout=0.1,\n",
    "        head_num=8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(BertNSP, self).__init__(**kwargs)\n",
    "\n",
    "        self.emb = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            seq_length=seq_length,\n",
    "            layer_num=layer_num,\n",
    "            model_dim=model_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            dropout=dropout,\n",
    "            head_num=head_num,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense = tf.keras.layers.Dense(2, activation=tf.keras.activations.softmax)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        emb = self.dropout(self.emb(inputs, training=training))\n",
    "        # shape [batch_size, 2]\n",
    "        return self.dense(emb[:, 0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5e6b5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 31), dtype=float32, numpy=\n",
       "array([[[0.0104665 , 0.02893532, 0.00224435, 0.00421711, 0.05016666,\n",
       "         0.0070851 , 0.02275247, 0.02934286, 0.03873889, 0.05066179,\n",
       "         0.0135875 , 0.00433905, 0.02290054, 0.0051624 , 0.19703811,\n",
       "         0.01757844, 0.00519997, 0.00181911, 0.00170542, 0.02262172,\n",
       "         0.01330194, 0.00624211, 0.14401814, 0.01847748, 0.00532914,\n",
       "         0.00253174, 0.00186335, 0.02260299, 0.22262819, 0.01252946,\n",
       "         0.01391215],\n",
       "        [0.01009972, 0.02907079, 0.00230911, 0.00433553, 0.05026842,\n",
       "         0.00702415, 0.0230408 , 0.02889612, 0.03803893, 0.05250223,\n",
       "         0.01317527, 0.0045732 , 0.02294951, 0.00522047, 0.20489928,\n",
       "         0.01718402, 0.00518294, 0.00172164, 0.00181506, 0.02320082,\n",
       "         0.01289726, 0.006328  , 0.14048722, 0.01804808, 0.00526207,\n",
       "         0.00262906, 0.00182247, 0.02258459, 0.21790743, 0.01211095,\n",
       "         0.01441482],\n",
       "        [0.01121071, 0.04354887, 0.00489386, 0.00732061, 0.05677856,\n",
       "         0.01013794, 0.03909738, 0.03551123, 0.02960102, 0.03456771,\n",
       "         0.01593554, 0.00580597, 0.00968067, 0.00311317, 0.2053747 ,\n",
       "         0.0218648 , 0.00452262, 0.00389008, 0.00172829, 0.05733465,\n",
       "         0.0158146 , 0.00662369, 0.09633043, 0.02102827, 0.0058363 ,\n",
       "         0.00530731, 0.0054565 , 0.02376944, 0.18263087, 0.01671647,\n",
       "         0.01856782],\n",
       "        [0.01121071, 0.04354887, 0.00489386, 0.00732061, 0.05677856,\n",
       "         0.01013794, 0.03909738, 0.03551123, 0.02960102, 0.03456771,\n",
       "         0.01593554, 0.00580597, 0.00968067, 0.00311317, 0.2053747 ,\n",
       "         0.0218648 , 0.00452262, 0.00389008, 0.00172829, 0.05733465,\n",
       "         0.0158146 , 0.00662369, 0.09633043, 0.02102827, 0.0058363 ,\n",
       "         0.00530731, 0.0054565 , 0.02376944, 0.18263087, 0.01671647,\n",
       "         0.01856782],\n",
       "        [0.01121071, 0.04354887, 0.00489386, 0.00732061, 0.05677856,\n",
       "         0.01013794, 0.03909738, 0.03551123, 0.02960102, 0.03456771,\n",
       "         0.01593554, 0.00580597, 0.00968067, 0.00311317, 0.2053747 ,\n",
       "         0.0218648 , 0.00452262, 0.00389008, 0.00172829, 0.05733465,\n",
       "         0.0158146 , 0.00662369, 0.09633043, 0.02102827, 0.0058363 ,\n",
       "         0.00530731, 0.0054565 , 0.02376944, 0.18263087, 0.01671647,\n",
       "         0.01856782]],\n",
       "\n",
       "       [[0.00823561, 0.03774346, 0.00201054, 0.00793342, 0.03735577,\n",
       "         0.01106599, 0.0245752 , 0.03021713, 0.01629054, 0.02688563,\n",
       "         0.007222  , 0.00179572, 0.08777223, 0.01399868, 0.19780575,\n",
       "         0.01880041, 0.01077185, 0.00279936, 0.0026596 , 0.02675561,\n",
       "         0.00827687, 0.08795764, 0.06332679, 0.00429383, 0.00591625,\n",
       "         0.00558839, 0.00197948, 0.03816464, 0.18908729, 0.00877579,\n",
       "         0.01393847],\n",
       "        [0.00797456, 0.03776439, 0.00195747, 0.00790341, 0.03768567,\n",
       "         0.01156515, 0.02483088, 0.02897695, 0.01593759, 0.02721988,\n",
       "         0.00714481, 0.0017562 , 0.09030934, 0.01407685, 0.20789945,\n",
       "         0.01916356, 0.01083884, 0.00279081, 0.00278584, 0.0272348 ,\n",
       "         0.00799003, 0.08319167, 0.06033899, 0.00414729, 0.00601849,\n",
       "         0.00533311, 0.00191963, 0.03804008, 0.1841841 , 0.00827493,\n",
       "         0.01474523],\n",
       "        [0.00756305, 0.04271098, 0.00314902, 0.00895868, 0.03337387,\n",
       "         0.01150563, 0.05985124, 0.02524316, 0.00999546, 0.02137276,\n",
       "         0.00774464, 0.00214158, 0.03294625, 0.01105866, 0.23541142,\n",
       "         0.02049458, 0.00870133, 0.00443784, 0.00223179, 0.08130234,\n",
       "         0.00899553, 0.10420774, 0.03327696, 0.00491889, 0.0065523 ,\n",
       "         0.00984966, 0.0033907 , 0.0300887 , 0.14888817, 0.00711687,\n",
       "         0.01252024],\n",
       "        [0.00756305, 0.04271098, 0.00314902, 0.00895868, 0.03337387,\n",
       "         0.01150563, 0.05985124, 0.02524316, 0.00999546, 0.02137276,\n",
       "         0.00774464, 0.00214158, 0.03294625, 0.01105866, 0.23541142,\n",
       "         0.02049458, 0.00870133, 0.00443784, 0.00223179, 0.08130234,\n",
       "         0.00899553, 0.10420774, 0.03327696, 0.00491889, 0.0065523 ,\n",
       "         0.00984966, 0.0033907 , 0.0300887 , 0.14888817, 0.00711687,\n",
       "         0.01252024],\n",
       "        [0.00756305, 0.04271098, 0.00314902, 0.00895868, 0.03337387,\n",
       "         0.01150563, 0.05985124, 0.02524316, 0.00999546, 0.02137276,\n",
       "         0.00774464, 0.00214158, 0.03294625, 0.01105866, 0.23541142,\n",
       "         0.02049458, 0.00870133, 0.00443784, 0.00223179, 0.08130234,\n",
       "         0.00899553, 0.10420774, 0.03327696, 0.00491889, 0.0065523 ,\n",
       "         0.00984966, 0.0033907 , 0.0300887 , 0.14888817, 0.00711687,\n",
       "         0.01252024]]], dtype=float32)>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = BertMLM(len(_VOCAB), _MAX_SEQ_LEN)\n",
    "mlm(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fa632ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.33159786, 0.66840214],\n",
       "       [0.36663258, 0.6333674 ]], dtype=float32)>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp = BertNSP(len(_VOCAB), _MAX_SEQ_LEN)\n",
    "nsp(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2ac858dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False, reduction=\"none\"\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "    \n",
    "    # mask indices where label == 0 (padding)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    # get the prediction index for target token\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f998761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(tf.keras.Model):\n",
    "\n",
    "    def call(self, inputs, training=False) -> tf.Tensor:\n",
    "        logits = mlm(inputs, training=training)\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "df0f4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "eb35dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.compile(\n",
    "            loss=masked_loss,\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.0001,\n",
    "            ),\n",
    "            metrics=[masked_accuracy],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6b702cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 13s 11ms/step - loss: 3.9962 - masked_accuracy: 0.0512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7568caffd0>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.fit(dataset.repeat(1000).map(lambda x: (x, x[\"masked_lm_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45ee89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

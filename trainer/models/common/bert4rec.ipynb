{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42914d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 11:16:08.214985: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 11:16:10.765646: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools\n",
    "print(\"TensorFlow version: \", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7115cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 11:16:27.052988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:27.350410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:27.350923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:27.396637: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 11:16:27.413600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:27.415590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:27.417491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:31.351814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:31.366082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:31.366432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-21 11:16:31.366774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7625 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text_a': <tf.Tensor: shape=(), dtype=string, numpy=b'Sponge bob Squarepants is an Avenger'>,\n",
       " 'text_b': <tf.Tensor: shape=(), dtype=string, numpy=b'Barack Obama is the President.'>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = {\n",
    "    \"text_a\": [\n",
    "      \"Sponge bob Squarepants is an Avenger\",\n",
    "      \"Marvel Avengers\"\n",
    "    ],\n",
    "    \"text_b\": [\n",
    "     \"Barack Obama is the President.\",\n",
    "     \"President is the highest office\"\n",
    "  ],\n",
    "}\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(examples)\n",
    "next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e7d46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_VOCAB = [\n",
    "    # Special tokens\n",
    "    b\"[UNK]\", b\"[MASK]\", b\"[RANDOM]\", b\"[CLS]\", b\"[SEP]\",\n",
    "    # Suffixes\n",
    "    b\"##ack\", b\"##ama\", b\"##ger\", b\"##gers\", b\"##onge\", b\"##pants\",  b\"##uare\",\n",
    "    b\"##vel\", b\"##ven\", b\"an\", b\"A\", b\"Bar\", b\"Hates\", b\"Mar\", b\"Ob\",\n",
    "    b\"Patrick\", b\"President\", b\"Sp\", b\"Sq\", b\"bob\", b\"box\", b\"has\", b\"highest\",\n",
    "    b\"is\", b\"office\", b\"the\",\n",
    "]\n",
    "\n",
    "_START_TOKEN = _VOCAB.index(b\"[CLS]\")\n",
    "_END_TOKEN = _VOCAB.index(b\"[SEP]\")\n",
    "_MASK_TOKEN = _VOCAB.index(b\"[MASK]\")\n",
    "_RANDOM_TOKEN = _VOCAB.index(b\"[RANDOM]\")\n",
    "_UNK_TOKEN = _VOCAB.index(b\"[UNK]\")\n",
    "_MAX_SEQ_LEN = 8\n",
    "_MAX_PREDICTIONS_PER_BATCH = 5\n",
    "\n",
    "_VOCAB_SIZE = len(_VOCAB)\n",
    "\n",
    "lookup_table = tf.lookup.StaticVocabularyTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=_VOCAB,\n",
    "      key_dtype=tf.string,\n",
    "      values=tf.range(\n",
    "          tf.size(_VOCAB, out_type=tf.int64), dtype=tf.int64),\n",
    "          value_dtype=tf.int64\n",
    "        ),\n",
    "      num_oov_buckets=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ccc101",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selector = text.RandomItemSelector(\n",
    "    max_selections_per_batch=_MAX_PREDICTIONS_PER_BATCH,\n",
    "    selection_rate=0.2,\n",
    "    unselectable_ids=[_START_TOKEN, _END_TOKEN, _UNK_TOKEN]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42533149",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_values_chooser = text.MaskValuesChooser(_VOCAB_SIZE, _MASK_TOKEN, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8b0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_pretrain_preprocess(vocab_table, features):\n",
    "    # Input is a string Tensor of documents, shape [batch, 1].\n",
    "    text_a = features[\"text_a\"]\n",
    "    text_b = features[\"text_b\"]\n",
    "\n",
    "    # Tokenize segments to shape [num_sentences, (num_words)] each.\n",
    "    tokenizer = text.BertTokenizer(\n",
    "      vocab_table,\n",
    "      token_out_type=tf.int64)\n",
    "    segments = [tokenizer.tokenize(text).merge_dims(\n",
    "      1, -1) for text in (text_a, text_b)]\n",
    "\n",
    "    # Truncate inputs to a maximum length.\n",
    "    trimmer = text.RoundRobinTrimmer(max_seq_length=6)\n",
    "    trimmed_segments = trimmer.trim(segments)\n",
    "\n",
    "    # Combine segments, get segment ids and add special tokens.\n",
    "    segments_combined, segment_ids = text.combine_segments(\n",
    "      trimmed_segments,\n",
    "      start_of_sequence_id=_START_TOKEN,\n",
    "      end_of_segment_id=_END_TOKEN)\n",
    "\n",
    "    # Apply dynamic masking task.\n",
    "    masked_input_ids, masked_lm_positions, masked_lm_ids = (\n",
    "      text.mask_language_model(\n",
    "        segments_combined,\n",
    "        random_selector,\n",
    "        mask_values_chooser,\n",
    "      )\n",
    "    )\n",
    "\n",
    "    # Prepare and pad combined segment inputs\n",
    "    input_word_ids, input_mask = text.pad_model_inputs(\n",
    "    masked_input_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "    input_type_ids, _ = text.pad_model_inputs(\n",
    "    segment_ids, max_seq_length=_MAX_SEQ_LEN)\n",
    "\n",
    "    # Prepare and pad masking task inputs\n",
    "    masked_lm_positions, masked_lm_weights = text.pad_model_inputs(\n",
    "    masked_lm_positions, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "    masked_lm_ids, _ = text.pad_model_inputs(\n",
    "    masked_lm_ids, max_seq_length=_MAX_PREDICTIONS_PER_BATCH)\n",
    "\n",
    "    model_inputs = {\n",
    "      \"input_word_ids\": input_word_ids,\n",
    "      \"input_mask\": input_mask,\n",
    "      \"input_type_ids\": input_type_ids,\n",
    "      \"masked_lm_ids\": masked_lm_ids,\n",
    "      \"masked_lm_positions\": masked_lm_positions,\n",
    "      \"masked_lm_weights\": masked_lm_weights,\n",
    "    }\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd4d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/fan/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_word_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[ 3, 22,  1, 24,  4, 27,  5, 19],\n",
       "        [ 3, 18,  1, 15,  4, 21, 28, 17]])>,\n",
       " 'input_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])>,\n",
       " 'input_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy=\n",
       " array([[0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1]])>,\n",
       " 'masked_lm_ids': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[ 9, 16,  0,  0,  0],\n",
       "        [12, 30,  0,  0,  0]])>,\n",
       " 'masked_lm_positions': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[2, 5, 0, 0, 0],\n",
       "        [2, 7, 0, 0, 0]])>,\n",
       " 'masked_lm_weights': <tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       " array([[1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensors(examples)\n",
    "    .map(functools.partial(bert_pretrain_preprocess, lookup_table))\n",
    ")\n",
    "\n",
    "next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4002ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"Bert embedding is composed of a positional embedding layer, a segment embedding layer and a normal embedding layer\n",
    "\n",
    "    Input shape\n",
    "      - token index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "      - segment index 2D tensor with shape: ``(batch_size, sequence_length)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, seq_length=128, dim=512, **kwargs):\n",
    "        super(BertEmbedding, self).__init__(**kwargs)\n",
    "        self.length = seq_length\n",
    "        self.dim = dim\n",
    "        self.token_emb = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=dim, mask_zero=True\n",
    "        )\n",
    "        self.position_emb = tf.keras.layers.Embedding(input_dim=seq_length, output_dim=dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        tokens = inputs[\"input_word_ids\"]\n",
    "        length = tf.shape(tokens)[1]\n",
    "        embedded_tokens = self.token_emb(tokens)\n",
    "        embedded_positions = self.position_emb(tf.range(length))\n",
    "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "        embedded_tokens *= tf.math.sqrt(tf.cast(self.dim, tf.float32))\n",
    "        return (\n",
    "            embedded_tokens + embedded_positions[tf.newaxis, :, :]\n",
    "        )\n",
    "\n",
    "    # Pass mask from token_emb, https://www.tensorflow.org/guide/keras/understanding_masking_and_padding#supporting_masking_in_your_custom_layers\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs[\"input_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3e605d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertEmbedding(len(_VOCAB), _MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bbc852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8, 512), dtype=float32, numpy=\n",
       "array([[[ 0.28828734, -0.9332832 , -0.7811977 , ..., -0.6922506 ,\n",
       "          0.77401567,  0.93710345],\n",
       "        [ 0.2757414 , -0.69747627, -0.72298133, ..., -1.0327996 ,\n",
       "          0.283246  , -0.0515362 ],\n",
       "        [-0.69469255, -0.20174801,  0.39593172, ...,  0.3419567 ,\n",
       "          1.0862778 , -1.0407891 ],\n",
       "        ...,\n",
       "        [-0.8904924 ,  1.0859271 , -0.75398684, ..., -0.8904877 ,\n",
       "          0.41478336,  1.0157971 ],\n",
       "        [-0.33193454,  1.1266761 , -0.00971618, ...,  0.07721391,\n",
       "          0.38347268,  1.0570387 ],\n",
       "        [ 0.44425493,  0.85371727, -0.57546055, ..., -0.29008284,\n",
       "         -0.7427849 ,  0.32641366]],\n",
       "\n",
       "       [[ 0.28828734, -0.9332832 , -0.7811977 , ..., -0.6922506 ,\n",
       "          0.77401567,  0.93710345],\n",
       "        [-0.9132184 , -1.112237  ,  0.47554365, ..., -0.04587528,\n",
       "          0.47510335,  0.8591468 ],\n",
       "        [-0.69469255, -0.20174801,  0.39593172, ...,  0.3419567 ,\n",
       "          1.0862778 , -1.0407891 ],\n",
       "        ...,\n",
       "        [-0.08733425,  0.54106426, -0.14539495, ..., -0.28670746,\n",
       "         -0.81337386, -0.40011927],\n",
       "        [-0.76044005, -0.17813648,  0.35833412, ...,  0.37676108,\n",
       "          1.0555285 , -1.0740103 ],\n",
       "        [ 0.7948906 ,  0.8230949 , -0.80115324, ..., -0.9429106 ,\n",
       "         -0.9065094 , -0.42908588]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "857ee951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from trainer.models.common.transformer import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1d92073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(tf.keras.layers.Layer):\n",
    "    \"\"\"Bert model is a stack of multiple layers of encoders\n",
    "\n",
    "    Input shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    References\n",
    "        - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        layer_num=6,\n",
    "        model_dim=512,\n",
    "        ff_dim=2048,\n",
    "        dropout=0.1,\n",
    "        head_num=8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Bert, self).__init__(**kwargs)\n",
    "        self.emb = BertEmbedding(\n",
    "            vocab_size=vocab_size, seq_length=seq_length, dim=model_dim\n",
    "        )\n",
    "        self.encoders = [\n",
    "            Encoder(\n",
    "                model_dim=model_dim, ff_dim=ff_dim, dropout=dropout, head_num=head_num\n",
    "            )\n",
    "            for _ in range(layer_num)\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # shape [batch_size, token_length]\n",
    "        emb = self.emb(inputs, training=training)\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        for encoder in self.encoders:\n",
    "            emb = encoder(emb, training=training)\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        return emb\n",
    "\n",
    "\n",
    "class Bert4Rec(tf.keras.layers.Layer):\n",
    "    \"\"\"Masked language model simply mask some percentage of the input tokens at random, and then predict those masked tokens\n",
    "\n",
    "    Input shape\n",
    "      - 3D tensor with shape: ``(batch_size, sequence_length, embedding_size)``.\n",
    "\n",
    "    Output shape\n",
    "      - 3D tensor with shape: ``(batch_size, masked_positions, vocab_size)``.\n",
    "\n",
    "    References\n",
    "        - [BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer](https://arxiv.org/pdf/1904.06690.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        seq_length,\n",
    "        layer_num=6,\n",
    "        model_dim=512,\n",
    "        ff_dim=2048,\n",
    "        dropout=0.1,\n",
    "        head_num=8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Bert4Rec, self).__init__(**kwargs)\n",
    "\n",
    "        self.bert = Bert(\n",
    "            vocab_size=vocab_size,\n",
    "            seq_length=seq_length,\n",
    "            layer_num=layer_num,\n",
    "            model_dim=model_dim,\n",
    "            ff_dim=ff_dim,\n",
    "            dropout=dropout,\n",
    "            head_num=head_num,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # shape [vocab_size, embedding_size]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            model_dim, activation='gelu'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        emb = self.dropout(self.bert(inputs, training=training))\n",
    "        # shape [batch_size, token_length, model_dim]\n",
    "        emb = self.dense(emb)\n",
    "        # gather the corresponding logits per the masked_lm_positions\n",
    "        # shape [batch_size, masked_lm_positions, model_dim]\n",
    "        emb = tf.gather(emb, inputs[\"masked_lm_positions\"], axis=1, batch_dims=1)\n",
    "        # shape [batch_size, vocab_size, embedding_size]\n",
    "        input_emb_weights = self.bert.emb.token_emb.trainable_variables[0]\n",
    "        # shape [batch_size, masked_positions, vocab_size]\n",
    "        return tf.matmul(emb, input_emb_weights, transpose_b=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5e6b5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 31), dtype=float32, numpy=\n",
       "array([[[-1.204608  , -0.3435057 , -0.02068051, -0.23195934,\n",
       "         -0.04339382, -0.30372506,  0.3473095 , -0.05549674,\n",
       "          0.1678929 ,  0.18790339, -0.7526599 ,  0.10378174,\n",
       "         -0.22640207,  0.52210367,  0.12653294,  0.15099859,\n",
       "         -0.27592474, -0.21125063,  0.12434558,  0.2426585 ,\n",
       "          1.3169292 ,  0.5647949 ,  0.1033027 ,  0.63603616,\n",
       "         -0.5432624 ,  0.13483912,  0.20665452, -0.26990935,\n",
       "          0.01863711,  0.40742958, -0.14430434],\n",
       "        [-1.2091808 , -0.33642054, -0.0244432 , -0.22224677,\n",
       "         -0.0459226 , -0.30168685,  0.33974928, -0.07155635,\n",
       "          0.15827467,  0.19008195, -0.76364845,  0.09578253,\n",
       "         -0.22988907,  0.5317125 ,  0.12016733,  0.1648632 ,\n",
       "         -0.27785444, -0.20776488,  0.10678308,  0.24328533,\n",
       "          1.3316113 ,  0.5694369 ,  0.10107972,  0.6305535 ,\n",
       "         -0.53755385,  0.14121746,  0.2001932 , -0.2647997 ,\n",
       "          0.02041976,  0.39034614, -0.14517915],\n",
       "        [-1.1590179 , -0.31117225, -0.12261035, -0.15797125,\n",
       "         -0.06909916, -0.3641605 ,  0.30246034, -0.16218929,\n",
       "         -0.00133474,  0.25070527, -0.559969  , -0.05747703,\n",
       "         -0.18564859,  0.512726  ,  0.21072152,  0.04572476,\n",
       "         -0.2688595 , -0.25326654,  0.16497843,  0.17010993,\n",
       "          1.1959554 ,  0.51851034, -0.05189678,  0.59957063,\n",
       "         -0.40876216,  0.16277963,  0.22039886, -0.46241426,\n",
       "         -0.11253251,  0.24865654, -0.14524552],\n",
       "        [-1.1590179 , -0.31117225, -0.12261035, -0.15797125,\n",
       "         -0.06909916, -0.3641605 ,  0.30246034, -0.16218929,\n",
       "         -0.00133474,  0.25070527, -0.559969  , -0.05747703,\n",
       "         -0.18564859,  0.512726  ,  0.21072152,  0.04572476,\n",
       "         -0.2688595 , -0.25326654,  0.16497843,  0.17010993,\n",
       "          1.1959554 ,  0.51851034, -0.05189678,  0.59957063,\n",
       "         -0.40876216,  0.16277963,  0.22039886, -0.46241426,\n",
       "         -0.11253251,  0.24865654, -0.14524552],\n",
       "        [-1.1590179 , -0.31117225, -0.12261035, -0.15797125,\n",
       "         -0.06909916, -0.3641605 ,  0.30246034, -0.16218929,\n",
       "         -0.00133474,  0.25070527, -0.559969  , -0.05747703,\n",
       "         -0.18564859,  0.512726  ,  0.21072152,  0.04572476,\n",
       "         -0.2688595 , -0.25326654,  0.16497843,  0.17010993,\n",
       "          1.1959554 ,  0.51851034, -0.05189678,  0.59957063,\n",
       "         -0.40876216,  0.16277963,  0.22039886, -0.46241426,\n",
       "         -0.11253251,  0.24865654, -0.14524552]],\n",
       "\n",
       "       [[-0.8049684 , -0.2884773 , -0.4870903 , -0.02996796,\n",
       "          0.05669811, -0.02721521,  0.30208778, -0.02739213,\n",
       "          0.0850485 ,  0.54937005, -0.99405426,  0.01502435,\n",
       "         -0.20716058,  0.7883527 ,  0.0976069 ,  0.16399789,\n",
       "         -0.32977456,  0.22372821,  0.36811224,  0.47021863,\n",
       "          0.85607994,  0.15879479, -0.17196196,  0.6904386 ,\n",
       "         -0.586007  , -0.17219889, -0.38456336, -0.562405  ,\n",
       "          0.26518613,  0.33341718, -0.22639012],\n",
       "        [-0.8032811 , -0.2937871 , -0.47399393, -0.03774999,\n",
       "          0.04610044, -0.03156427,  0.30365583, -0.0371096 ,\n",
       "          0.09408647,  0.5431733 , -1.0089774 ,  0.02075846,\n",
       "         -0.20496354,  0.800082  ,  0.08896008,  0.17844886,\n",
       "         -0.33180952,  0.2273699 ,  0.36665875,  0.47012067,\n",
       "          0.86842585,  0.17492633, -0.1779129 ,  0.6858928 ,\n",
       "         -0.5787508 , -0.18098255, -0.3781753 , -0.57210404,\n",
       "          0.25795445,  0.32821497, -0.2139446 ],\n",
       "        [-0.72737134, -0.45775098, -0.60317194,  0.10934434,\n",
       "          0.08666304,  0.18746741,  0.34063256, -0.26646256,\n",
       "          0.07528555,  0.4965968 , -0.606141  , -0.09024783,\n",
       "         -0.3053866 ,  0.8801612 ,  0.27646136,  0.03551435,\n",
       "         -0.4467441 ,  0.05800901,  0.5079857 ,  0.28251454,\n",
       "          0.7682788 ,  0.1705229 , -0.27198136,  0.7897101 ,\n",
       "         -0.6776192 , -0.17465037, -0.19003901, -0.73730814,\n",
       "          0.10383987,  0.16743088, -0.18467115],\n",
       "        [-0.72737134, -0.45775098, -0.60317194,  0.10934434,\n",
       "          0.08666304,  0.18746741,  0.34063256, -0.26646256,\n",
       "          0.07528555,  0.4965968 , -0.606141  , -0.09024783,\n",
       "         -0.3053866 ,  0.8801612 ,  0.27646136,  0.03551435,\n",
       "         -0.4467441 ,  0.05800901,  0.5079857 ,  0.28251454,\n",
       "          0.7682788 ,  0.1705229 , -0.27198136,  0.7897101 ,\n",
       "         -0.6776192 , -0.17465037, -0.19003901, -0.73730814,\n",
       "          0.10383987,  0.16743088, -0.18467115],\n",
       "        [-0.72737134, -0.45775098, -0.60317194,  0.10934434,\n",
       "          0.08666304,  0.18746741,  0.34063256, -0.26646256,\n",
       "          0.07528555,  0.4965968 , -0.606141  , -0.09024783,\n",
       "         -0.3053866 ,  0.8801612 ,  0.27646136,  0.03551435,\n",
       "         -0.4467441 ,  0.05800901,  0.5079857 ,  0.28251454,\n",
       "          0.7682788 ,  0.1705229 , -0.27198136,  0.7897101 ,\n",
       "         -0.6776192 , -0.17465037, -0.19003901, -0.73730814,\n",
       "          0.10383987,  0.16743088, -0.18467115]]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = Bert4Rec(len(_VOCAB), _MAX_SEQ_LEN)\n",
    "mlm(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ac858dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False, reduction=\"none\"\n",
    "    )\n",
    "    loss = loss_object(label, pred)\n",
    "    \n",
    "    # mask indices where label == 0 (padding)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    # get the prediction index for target token\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    match = match & mask\n",
    "\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f998761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(tf.keras.Model):\n",
    "\n",
    "    def call(self, inputs, training=False) -> tf.Tensor:\n",
    "        logits = mlm(inputs, training=training)\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df0f4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb35dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.compile(\n",
    "            loss=masked_loss,\n",
    "            optimizer=tf.keras.optimizers.Adam(\n",
    "                learning_rate=0.0001,\n",
    "            ),\n",
    "            metrics=[masked_accuracy],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b702cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 16s 13ms/step - loss: 10.9292 - masked_accuracy: 7.5000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57d01d2400>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.fit(dataset.repeat(1000).map(lambda x: (x, x[\"masked_lm_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45ee89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
